## E-COMMERCE PROJECT
### Project Overview
The e-commerce project investigates the progress of sales, orders, and customers buying through a website and their buying patterns over the whole period. This project aims to understand the way products move around on this website and how they can be optimized better.

### Objectives
 How many Goods are Sold Per Hour?

	Which Customers have the most orders?

	Which Customers spend the most amount?

	Which are the most frequent items bought? 

	Which is the Most expensive purchase made? 

	Which Date had the most Orders?

	Find the Country whose Sales were better than the average sales of the Other Country

	The Sales shown are the sales above the average Sales in each Country

	Find the Sales by Country.

	Find the Goods Sold by Country

	How many Customers are there in each Country?

	Which Country has the most quantity sold?

	Which are the Highest Sales In each country?

### Data sources
This project was found on Kaggle. It entails the data of sales of an electronic commerce business. It has sales of countries, the invoice of sales made, the orders, it shows hours with most sales. It has over eighty thousand unique records.

### Data Cleaning/Preparation

**Data Filtering**-The dataset was filtered by selectively extracting specific information dataset. It involved applying rules or conditions to the data to isolate relevant subsets and enhance its usability for analysis or presentation. The filtering process can be performed using various methods, such as using software tools

**Data Deduplication** involves removing duplicate records or entries from a dataset to maintain data accuracy and consistency. The duplicate values were identified and removed duplicate files within the storage system. The process involved scanning data for duplicate segments, replaced with pointers to the unique data copy.

**Data cleaning**- It involved Handling Missing data and identifying and dealing with missing values through imputation or removal. Removing Duplicate Entries and eliminating identical or redundant records to avoid distortions in analysis. Standardizing Formats Ensuring consistency in data formats for uniformity. Correcting Inaccuracies Identifying and rectifying inaccuracies, outliers, or anomalies that could skew results. Handling Typos and misspellings, Correcting errors in data entry, such as typos and misspellings, to maintain accuracy. Dealing with Inconsistent Data Resolving inconsistencies in categorical data or addressing conflicting information Data Outliers Identifying and managing outliers that may impact statistical analyses.


### Exploratory Data Analysis
I have been able to Maximize insight into this data set, uncover underlying structure, extract important variables, detect outliers and anomalies, Test underlying assumptions, and Determine optimal factor settings.
The main purpose was to open-mindedly explore the data, often without having a specific hypothesis in mind, to discover patterns, spot anomalies, test a hypothesis, or check assumptions with the help of summary statistics and graphical representations. 

Rush hour occurs around 11-15 when the site sells more goods than other hours. The slowest periods are towards the end of the day and the beginning of the day. The 12th hour has the most traffic and sales.

A lot of customers and sales around the end of the year with November and December having massive sales and traffic 
But throughout the end of 2010, it picked up and the activity has remained high throughout 2011 with an estimation of 200 a day.

